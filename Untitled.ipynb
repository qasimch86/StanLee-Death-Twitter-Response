{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess(raw_text):\n",
    "\n",
    "    # keep only words\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "\n",
    "    # convert to lower case and split \n",
    "    words = letters_only_text.lower().split()\n",
    "\n",
    "    # remove stopwords\n",
    "    stopword_set = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if w not in stopword_set]\n",
    "\n",
    "    # join the cleaned words in a list\n",
    "    cleaned_word_list = \" \".join(meaningful_words)\n",
    "\n",
    "    return cleaned_word_list\n",
    "\n",
    "def process_data(dataset):\n",
    "    tweets_df = pd.read_csv(dataset,delimiter='|',header=None)\n",
    "\n",
    "    num_tweets = tweets_df.shape[0]\n",
    "    print(\"Total tweets: \" + str(num_tweets))\n",
    "\n",
    "    cleaned_tweets = []\n",
    "    print(\"Beginning processing of tweets at: \" + str(datetime.now()))\n",
    "\n",
    "    for i in range(num_tweets):\n",
    "        cleaned_tweet = preprocess(tweets_df.iloc[i][1])\n",
    "        cleaned_tweets.append(cleaned_tweet)\n",
    "        if(i % 10000 == 0):\n",
    "            print(str(i) + \" tweets processed\")\n",
    "\n",
    "    print(\"Finished processing of tweets at: \" + str(datetime.now()))\n",
    "    return cleaned_tweets\n",
    "\n",
    "cleaned_data = process_data(\"tweets.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column you are working on\n",
    "df_ = tweets_df[1]\n",
    "\n",
    "stopword_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "# convert to lower case and split \n",
    "df_ = df_.str.lower().split()\n",
    "\n",
    "# remove stopwords\n",
    "df_ = df_.apply(lambda x: [item for item in x if item not in stopword_set])\n",
    "\n",
    "# keep only words\n",
    "regex_pat = re.compile(r'[^a-zA-Z\\s]', flags=re.IGNORECASE)\n",
    "df_ = df_.str.replace(regex_pat, '')\n",
    "\n",
    "# join the cleaned words in a list\n",
    "df_.str.join(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess2(raw_text):\n",
    "    stopword_set = set(stopwords.words(\"english\"))\n",
    "    return \" \".join([i for i in re.sub(r'[^a-zA-Z\\s]', \"\", raw_text).lower().split() if i not in stopword_set])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
